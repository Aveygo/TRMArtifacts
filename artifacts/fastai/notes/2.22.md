# Lesson 22: Diffusion Models and Beyond

## Introduction
- Jeremy welcomes Johno and Tanishq for Lesson 22.
- Focus on advancing diffusion models, building on Lesson 21.

## Recap and Goals
- Previous success: high-quality Fashion-MNIST generation with DDPM/DDIM.
- Aim: explore UNet architectures and faster sampling techniques.
- Plan to go beyond Stable Diffusion with new research directions.

## UNet Architecture Overview
- UNet: critical for diffusion models, handles noise prediction.
- Structure: encoder-decoder with skip connections for detail preservation.
- Goal: understand and customize UNet for Fashion-MNIST.

## Improving Sampling Efficiency
- DDIM revisited: faster sampling with fewer steps.
- Experiment: varying step sizes to balance speed and quality.
- Challenge: maintaining FID while reducing computation time.

## Exploring Cosine Schedules
- Cosine schedule preferred over linear for smoother noise addition.
- Benefits: avoids flat regions, improves sampling stability.
- Comparison: cosine vs. adjusted linear (betamax=0.01) schedules.

## Experiment Tracking Enhancements
- Weights and Biases (W&B) for logging metrics, samples, and configs.
- New callback ideas: log intermediate samples, automate hyperparameter tracking.
- Discussion: balancing automation with directed experimentation.

## FID and KID Metrics Refinement
- FID used consistently with Fashion-MNIST classifier for accuracy.
- KID less reliable due to high variance, FID preferred.
- Issue: FID sensitivity to sample size and preprocessing.

## New Research Directions
- Beyond Stable Diffusion: exploring latent diffusion and score-based models.
- Idea: integrate attention mechanisms into UNet for better detail.
- Plan: test on CIFAR-10, then scale to larger datasets.

## Challenges with CIFAR-10
- CIFAR-10â€™s low resolution complicates visual evaluation.
- Need for robust metrics to assess generated image quality.
- Strategy: combine human evaluation with automated metrics.

## Conclusion
- Progress: optimized UNet and sampling for Fashion-MNIST.
- Next steps: implement attention-based UNet, test on larger datasets.
- Emphasis: rigorous experimentation with clear hypotheses.