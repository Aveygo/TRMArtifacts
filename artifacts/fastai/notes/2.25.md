# Lesson 25: Stable Diffusion and VAEs

## Introduction
- Final lesson of part two, focusing on stable diffusion.
- Clip embeddings deferred to next part (NLP focus).
- Plan to explore GPT-4 and NLP deeply due to recent advancements.

## Audio Diffusion with Spectrograms
- Applied pixel-level diffusion to audio (bird calls dataset).
- Dataset: Subset from Xenocanto, 10 bird call classes, 32kHz sample rate.
- Challenge: Raw audio (1D array) too large, complex for modeling.
- Solution: Convert to Mel spectrogram (128x128 grayscale image).
- Spectrogram: Maps time (x-axis) and frequency (y-axis), tuned for human hearing.
- Used `diffusers.pipelines.audiodiffusion.Mel` for waveform-to-spectrogram conversion.
- Lossy conversion: Spectrogram lacks phase info, uses Griffin-Lim algorithm for reconstruction.
- Model: Simple diffusion (copied from notebook 30), trained 15 epochs.
- Results: Generated spectrograms converted back to audio, some recognizable bird calls.

## Variational Autoencoder (VAE) Basics
- Goal: Compress images into latent representations for efficient processing.
- Dataset: Fashion MNIST, flattened to 784D vectors.
- Autoencoder: Encoder (784→400→200), decoder (200→400→784), trained with BCE loss.
- Results: Rough reconstruction, poor random noise decoding.
- VAE modification: Encoder splits into mu and log-variance layers.
- VAE process: Adds randomness to latents using mu and log-variance.
- Loss: BCE + KL divergence (ensures mu ≈ 0, log-variance ≈ 1).
- Results: Worse reconstruction but better random noise decoding (recognizable shapes).

## Stable Diffusion VAE
- Dataset: LSUN bedrooms (subset hosted on AWS, 256x256 images).
- Pre-trained VAE: Compresses 256x256x3 to 32x32x4 (48x smaller).
- Encoding/decoding: High-quality reconstruction, minor issues with text/faces.
- Training: Used VAE latents, simple diffusion model, 25 epochs (single GPU).
- Results: Decent bedroom images, improved with 100 epochs (15 hours on A100).
- Loss: Perceptual + adversarial (GAN) for sharper outputs.

## Pre-trained Latent Classifier
- Idea: Train classifier on VAE latents for potential backbone use.
- Dataset: ImageNet (1.3M images), converted to latents.
- Method: Cached file list, saved latents as separate NumPy files.
- Model: ResNet-like with pre-activation convolutions, trained 40 epochs.
- Results: 66% accuracy (comparable to ResNet-34 at 73-74%).

## Future Directions
- Experiment with pre-trained latent backbones for diffusion models.
- Try CelebA-HQ dataset for face generation (good results shown).
- Combine domain expertise with diffusion techniques.
- Join FastAI Discord for collaboration and feedback.
- Explore research ideas, implement recent papers.

## Conclusion
- Course completion: Strong foundation in diffusion models.
- Next steps: NLP focus, potential GPT-4 exploration.
- Encouragement: Experiment, collaborate, apply skills to real-world problems.