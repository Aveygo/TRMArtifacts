# Lesson 24: Stable Diffusion from Scratch

## Introduction
- Lesson 24 focuses on completing conditional Stable Diffusion (except CLIP).
- Joined by Jono and Tanishq for discussion.
- Goal: Build unconditional and conditional diffusion models from scratch.

## Unconditional Diffusion Unit
- Implements a UNet based on diffusers' structure.
- Uses pre-activation convolution (normalization and activation before convolution).
- ResNet block without downsampling; downsampling handled in down block with stride-2 convolution.
- Down block: Sequential saved ResNet blocks and optional stride-2 convolution.
- Up block: Similar to down block, uses upsampling layer followed by stride-1 convolution.
- Saved modules (mix-in pattern) store activations for UNet skip connections.
- Trained on Fashion MNIST with fewer channels than default Stable Diffusion.

## Time Embeddings
- Adds sinusoidal time embeddings to improve sampling.
- Time steps represented as vectors, similar to NLP token embeddings.
- Embeddings ensure similar sigmas have similar vectors using sine/cosine functions.
- Max period (10,000) borrowed from NLP; may be wasteful for diffusion.
- Time embeddings projected via linear layer, split for scale/shift operations in ResNet blocks.

## Attention Mechanism
- Implements 1D self-attention as in Stable Diffusion.
- Flattens image pixels into a sequence for NLP-style attention.
- Uses three projections (Q, K, V) to compute weighted averages of pixels.
- Multi-headed attention splits channels into groups for independent attention.
- Uses `einops.rearrange` for efficient tensor reshaping.
- Attention added to ResNet blocks at later layers (e.g., 16x16 or 32x32) to manage memory.
- No significant performance improvement observed with attention in this case.

## Conditional Diffusion Model
- Extends model to condition on Fashion MNIST class labels (e.g., t-shirt, pants).
- Adds label embedding (same size as time embedding) and sums with time embedding.
- Modifies data collation to include labels; model input includes activations, time steps, and labels.
- Conditional sampling specifies desired class ID (e.g., 0 for t-shirt).
- Training loss slightly better due to additional class information.

## Transformers Discussion
- Transformer block: Attention + MLP with normalization.
- Diffusion modelâ€™s mid block replaced with Transformer blocks for experimentation.
- Vision Transformers (ViTs) require large datasets to outperform convolutions.
- Positional embeddings (sinusoidal) critical for ViTs; size-specific to input resolution.

## Next Steps
- Next lesson: Variational Autoencoders (VAEs) and latent diffusion.
- Possible inclusion of ongoing research findings.
- CLIP integration for text-conditioned diffusion to be addressed later.